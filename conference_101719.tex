\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\DeclareFontShape{JT2}{mc}{m}{sc}{<->ssub*mc/m/n}{}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[dvipdfmx]{graphicx}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{The New Fake News Detection with Generated Comments from News Article% *\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Yuta Yanagi}
\IEEEauthorblockA{\textit{Department of Informatics} \\
\textit{the University of Electro-Communications}\\
Tokyo, Japan \\
yanagi.yuta@ohsuga.lab.uec.ac.jp}
\and
\IEEEauthorblockN{Yasuyuki Tahara}
\IEEEauthorblockA{\textit{Department of Informatics} \\
\textit{the University of Electro-Communications}\\
Tokyo, Japan \\
tahara@uec.ac.jp}
\and
\IEEEauthorblockN{Yuichi Sei}
\IEEEauthorblockA{\textit{Department of Informatics} \\
\textit{the University of Electro-Communications}\\
Tokyo, Japan \\
sei@uec.ac.jp}
\and
\IEEEauthorblockN{Akihiko Ohsuga}
\IEEEauthorblockA{\textit{Department of Informatics} \\
\textit{the University of Electro-Communications}\\
Tokyo, Japan \\
ohsuga@uec.ac.jp}
}

\maketitle

\begin{abstract}
Recently, fake news spread via social networks and make the wrong rumor faster.
This problem is serious because the wrong rumor sometimes make social damage via deceived people.
Fact-checking is an ordinary solution to measure the credibility of news articles.
However this process usually takes a long time and it is hard to make it before spreading the wrong rumor.
Automatic detection of fake news is a popular researching topic.
It is confirmed that considering not only articles but also social context(i.e. likes, retweets, replies, comments, etc.) supports to spot fake news correctly by them.
This type is also hard to detect fake news before spreading the wrong rumor because social context is made with spreading on social networks.
We propose a fake news detector with generating part of the social context which is extended from the fake news generator model.
This model is trained about generating comments and classification of real/fake by dataset which is combined news and comments.
To measure this model's classification quality, we checked classification results from articles with real comments and generated ones by itself.
We compared results between classified with attached a generated comment and real comments only and we got results that considering a generated comment help detect more fake news than considering real comments only.
\end{abstract}

\begin{IEEEkeywords}
fake news, disinformation, neural network, natural language processing, deep-learning, microblogs
\end{IEEEkeywords}

\section{Introduction}
In this era, social media is one of the important parts of our lives.
Social media makes it easier to get news and share them with friends online.
However, at the same moment, 
there is also information that includes less credibility.
Some of them have obvious misinformation that is made by malicious purpose,
we call them ``fake news''.

Fake news try to make wrong rumors by spreading on social media.
This year, there is so much fake news about COVID-19 and sometimes these make wrong rumors.
Directer of General of the WHO called this problem ``infodemic'' and he told that fake news spreads faster and more easily than this virus\cite{ZAROCOSTAS2020676}. 
Besides, fake news created some mayhem not only online, but also offline (real incidents)
e.g. in Washington, fake news about the Pizzagate conspiracy is reported to have motivated the shooting\cite{agencies_2016}.
Spreading fake news also shakes the premise of democracy due to people cannot get accurate information.
Therefore, some researches try to spot fake news by machine learning.

The challenging point of this is there are news articles which try to deceive readers
and this makes harder to classify by simple rule-based method.
To get more information to detection,
there are some works which aggregate social context i.e. Retweet, Like, and comments
report better results than only considering news text\cite{Guo:2018:RDH:3269206.3271709}.
However, social contexts are not able to get before spreading.
Hence, there is also a work that generates words of comments from the news by CVAE to detect fake news when they are just posted\cite{ijcai2018-533}.
Their work tries to generate comments, but generated ones are only words that have a high probability of appearing.

In this work, we will propose a model that evaluates news credibility by news text and generated comments.
This model is modified from generating fake news articles\cite{NIPS2019_9106} and this model trains not only news features but also generating comments.
In training, this sequence includes real posted comments but the test sequence does not use them to simulate operation in real-social media.
The skill of generating comments help classification in the test sequence.

We measure the performance of our proposed method by some experiments with a real-posted dataset.

\section{Related works}
\subsection{Fact checking}
\subsection{Detecting fake news}
\subsection{Generating fake news}

\section{methodology}
\subsection{Generating comments}
\subsection{Classifying news and comments}

\section{results}
\subsection{Word generation tendency}
\label{subsec:trend}
First of all, we investigated the difference between generated comments from real and fake news.
We generated comments which refer to news articles that are fact-checked by PolitiFact.
This dataset contains sets of a news article and tweets(comments) which refer to it.
We filtered news which have at least three tweets and sampled three tweets for generating.
Both of real and fake labels have 200 sets of an article and comments and we trained to generate comments. 
We used these indexes: times of used words, percentage of used words, and the gap of a percentage point of used words of generated comments from real and fake.
We removed extra elements: stop words by NLTK, url(starts with \textit{http, https}), and part of symbols.
We didn't remove mentions, colons, and hashtags(i.e. @anyone, analyze: \#anything).
We found these features of all generated comments:
\begin{itemize}[\IEEEsetlabelwidth{3}]
    \item The most generated word was ``via''(c. 1.5\%).  
    \item ``via'' was also top frequency of generated word from both of real and fake. 
    \item The second and third were ``trump'' and ``obama'' but both of their percentages was under 1\%.
\end{itemize}
We also found the difference between generated comments from real and fake news.
\begin{itemize}[\IEEEsetlabelwidth{3}]
    \item The percent about frequency of ``via'' in generated comments from fake news article was twice as much as ones from real news.
    \item ``via'' was also the most gap of frequency between generated comments from real and fake.  The delta was c. 0.9 percentage point.
    \item ``breaking:'' was the second of the percentage point between frequency(fake was more than real). the delta was c. 0.7 percentage point.
\end{itemize}
\subsection{Quality of classify}
We measured effect of generated comment for classification by comparing classification without a generated comment.
We prepared baselines: classify by only news article, with two real-posted comments.
We used pair of an article of GossipCop and tweets instead because ones of PolitiFact were too few to make classification accurate.
We sampled same rule of \ref{subsec:trend} but both of real and fake labels have 2000 sets.
The result of classification is Table \ref{fig:classify_results}.
Our proposed method was best of recall score but precision is worse than consider without generated comments.
\begin{table}[!t]
    \renewcommand{\arraystretch}{1.3}
    \caption{Results of classification}
    \label{fig:classify_results}
    \centering
    \begin{tabular}{lccc}
    \hline
    Model name           & Precision & Recall & F1 score \\ \hline
    Article only         & 0.647     & 0.615  & 0.631    \\
     + Real comment * 2  & \textbf{0.682}     & 0.750  & \textbf{0.714}    \\
     + Generated comment & 0.590     & \textbf{0.790}  & 0.675    \\ \hline
    \end{tabular}
    \end{table}
\section{discussion}
\appendix[Settings of experiments]
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,myreferences}

\end{document}
